{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# -----------------------------------------------\n","# 1. Instalasi Streamlit dan Ngrok\n","# -----------------------------------------------\n","!pip install streamlit pyngrok\n","\n","# -----------------------------------------------\n","# 2. Tambahkan Authtoken Ngrok\n","# -----------------------------------------------\n","!ngrok config add-authtoken 2yGNfQjDh69wmZgxaX0U8FOoYKF_2gGrZkXHTzKcv72kRRYTS"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"5t9eiAxk3cbh","executionInfo":{"status":"ok","timestamp":1749649332844,"user_tz":-420,"elapsed":25469,"user":{"displayName":"Rofiq Samanhudi","userId":"07668947446064414586"}},"outputId":"2520bb43-9701-4e99-b0b5-640be59d1b5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n","Collecting pyngrok\n","  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n","Collecting watchdog<7,>=2.1.5 (from streamlit)\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m500.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n","Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 pyngrok-7.2.11 streamlit-1.45.1 watchdog-6.0.0\n","Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}]},{"cell_type":"code","source":["%%writefile clustering_app.py\n","import streamlit as st\n","import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import io\n","import kagglehub\n","import os\n","from typing import Tuple, Optional, Union, List\n","\n","# --- Helper Functions ---\n","def normalize_data(data: np.ndarray, method: str) -> Tuple[np.ndarray, Optional[object]]:\n","    \"\"\"Normalisasi data menggunakan Min-Max Scaling atau Z-Score.\"\"\"\n","    if method == \"Min-Max Scaling\":\n","        scaler = MinMaxScaler()\n","        normalized_data = scaler.fit_transform(data)\n","        return normalized_data, scaler\n","    elif method == \"Z-Score\":\n","        scaler = StandardScaler()\n","        normalized_data = scaler.fit_transform(data)\n","        return normalized_data, scaler\n","    return data, None\n","\n","def calculate_silhouette_score(data: np.ndarray, labels: np.ndarray) -> Optional[float]:\n","    \"\"\"Hitung rata-rata Skor Silhouette untuk mengevaluasi kualitas pengelompokan.\"\"\"\n","    try:\n","        unique_labels = np.unique(labels)\n","        if len(unique_labels) > 1:\n","            return silhouette_score(data, labels)\n","        return None\n","    except Exception as e:\n","        st.error(f\"[Error] Gagal menghitung Silhouette Score: {str(e)}\")\n","        return None\n","\n","def calculate_davies_bouldin_score(data: np.ndarray, labels: np.ndarray) -> Optional[float]:\n","    \"\"\"Hitung Indeks Davies-Bouldin untuk mengevaluasi kualitas pengelompokan.\"\"\"\n","    try:\n","        unique_labels = np.unique(labels)\n","        if len(unique_labels) > 1:\n","            return davies_bouldin_score(data, labels)\n","        return None\n","    except Exception as e:\n","        st.error(f\"[Error] Gagal menghitung Davies-Bouldin Index: {str(e)}\")\n","        return None\n","\n","def analisis_kmeans(data: np.ndarray, labels: np.ndarray, model: KMeans):\n","    \"\"\"Lakukan analisis K-Means dan sediakan distribusi klaster, metrik evaluasi, serta detail outlier.\"\"\"\n","    n_samples = len(data)\n","    cluster_counts = np.bincount(labels, minlength=model.n_clusters)\n","    st.write(f\"\\n*Distribusi Klaster dengan {n_samples} sampel:*\")\n","    dist_data = []\n","    for i, count in enumerate(cluster_counts):\n","        percentage = (count / n_samples) * 100 if n_samples > 0 else 0\n","        dist_data.append({\"Klaster\": f\"Klaster {i}\", \"Jumlah Sampel\": count, \"Persentase (%)\": percentage})\n","    st.dataframe(pd.DataFrame(dist_data), height=300, use_container_width=True)\n","\n","    inertia = model.inertia_\n","    silhouette = calculate_silhouette_score(data, labels)\n","    db_index = calculate_davies_bouldin_score(data, labels)\n","    st.write(\"\\n*Metrik Evaluasi Klaster:*\")\n","    metrics_data = [\n","        {\"Metrik\": \"Inertia (Within-Cluster Sum of Squares)\", \"Nilai\": f\"{inertia:.3f}\"},\n","        {\"Metrik\": \"Silhouette Score\", \"Nilai\": f\"{silhouette:.3f}\" if silhouette is not None else \"N/A\"},\n","        {\"Metrik\": \"Davies-Bouldin Index\", \"Nilai\": f\"{db_index:.3f}\" if db_index is not None else \"N/A\"}\n","    ]\n","    st.dataframe(pd.DataFrame(metrics_data), height=150, use_container_width=True)\n","\n","    distances = np.min(model.transform(data), axis=1)\n","    threshold = np.percentile(distances, 95) if len(distances) > 0 else np.inf\n","    outliers = np.where(distances > threshold)[0]\n","    outlier_percentage = (len(outliers) / n_samples) * 100 if n_samples > 0 else 0\n","    st.write(\"\\n*Outlier Analysis:*\")\n","    outlier_summary = [\n","        {\"Metrik\": \"Jumlah Outlier\", \"Nilai\": f\"{len(outliers)} sampel\"},\n","        {\"Metrik\": \"Persentase Outlier\", \"Nilai\": f\"{outlier_percentage:.2f}%\"}\n","    ]\n","    st.dataframe(pd.DataFrame(outlier_summary), height=100, use_container_width=True)\n","\n","    outlier_counts = np.bincount(labels[outliers], minlength=model.n_clusters) if len(outliers) > 0 else np.zeros(model.n_clusters)\n","    st.write(\"\\n*Distribusi Outlier per Klaster:*\")\n","    outlier_dist_data = []\n","    for i, count in enumerate(outlier_counts):\n","        percentage = (count / cluster_counts[i]) * 100 if cluster_counts[i] > 0 else 0\n","        outlier_dist_data.append({\"Klaster\": f\"Klaster {i}\", \"Jumlah Outlier\": count, \"Persentase dari Klaster (%)\": percentage})\n","    st.dataframe(pd.DataFrame(outlier_dist_data), height=200, use_container_width=True)\n","\n","    return outlier_counts, cluster_counts, outliers, distances[outliers]\n","\n","def perform_clustering(data: np.ndarray, k_value: int) -> Tuple[np.ndarray, Optional[object]]:\n","    \"\"\"Lakukan pengelompokan (clustering) menggunakan K-Means.\"\"\"\n","    try:\n","        model = KMeans(n_clusters=k_value, random_state=42, n_init=10)\n","        labels = model.fit_predict(data)\n","        return labels, model\n","    except Exception as e:\n","        st.error(f\"[Error] Gagal melakukan clustering: {str(e)}\")\n","        return np.array([]), None\n","\n","def get_clustering_diagnostics(labels: np.ndarray) -> int:\n","    \"\"\"Hitung jumlah klaster untuk K-Means.\"\"\"\n","    unique_labels = np.unique(labels)\n","    return len(unique_labels)\n","\n","def plot_silhouette_analysis(data: np.ndarray, labels: np.ndarray, normalization: str) -> Tuple[Optional[go.Figure], Optional[str]]:\n","    \"\"\"Buat plot siluet untuk analisis klaster.\"\"\"\n","    try:\n","        unique_labels = np.unique(labels)\n","        n_unique_labels = len(unique_labels)\n","        if n_unique_labels < 2:\n","            return None, f\"Hanya ditemukan {n_unique_labels} cluster. Dibutuhkan minimal 2 cluster untuk analisis silhouette.\"\n","\n","        silhouette_vals = silhouette_samples(data, labels)\n","        silhouette_avg = silhouette_score(data, labels)\n","\n","        y_lower = 10\n","        cluster_labels = sorted(unique_labels)\n","        fig = go.Figure()\n","        colors = px.colors.qualitative.Plotly\n","\n","        for i, cluster in enumerate(cluster_labels):\n","            cluster_silhouette_vals = silhouette_vals[labels == cluster]\n","            cluster_silhouette_vals.sort()\n","            size_cluster_i = cluster_silhouette_vals.shape[0]\n","            y_upper = y_lower + size_cluster_i\n","            y_vals = np.arange(y_lower, y_upper)\n","            fig.add_trace(go.Scatter(\n","                x=cluster_silhouette_vals,\n","                y=y_vals,\n","                mode='lines+markers',\n","                name=f'Cluster {cluster}',\n","                line=dict(color=colors[i % len(colors)], width=2),\n","                marker=dict(size=4)\n","            ))\n","            y_lower = y_upper + 10\n","\n","        fig.add_shape(\n","            type=\"line\",\n","            x0=silhouette_avg,\n","            x1=silhouette_avg,\n","            y0=0,\n","            y1=y_lower,\n","            line=dict(color=\"red\", dash=\"dash\")\n","        )\n","\n","        fig.add_annotation(\n","            x=silhouette_avg,\n","            y=y_lower,\n","            text=f\"Average Silhouette Score: {silhouette_avg:.4f}\",\n","            showarrow=True,\n","            arrowhead=1\n","        )\n","\n","        fig.update_layout(\n","            title=f\"Silhouette Analysis (K-Means dengan {normalization})\",\n","            xaxis_title=\"Silhouette Coefficient\",\n","            yaxis_title=\"Sample Index\",\n","            showlegend=True,\n","            xaxis=dict(range=[-0.1, 1.1]),\n","            yaxis=dict(tickvals=[], ticktext=[]),\n","            height=600\n","        )\n","\n","        return fig, None\n","    except Exception as e:\n","        return None, f\"Error saat membuat plot silhouette: {str(e)}\"\n","\n","def plot_elbow_method(data: np.ndarray, normalization: str) -> go.Figure:\n","    \"\"\"Lakukan Metode Elbow untuk menentukan nilai K yang optimal dan visualisasikan hasilnya.\"\"\"\n","    wcss = []\n","    for i in range(1, 11):\n","        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)\n","        kmeans.fit(data)\n","        wcss.append(kmeans.inertia_)\n","\n","    fig = go.Figure()\n","    fig.add_trace(go.Scatter(\n","        x=list(range(1, 11)),\n","        y=wcss,\n","        mode='lines+markers',\n","        name='WCSS',\n","        line=dict(color='blue', width=2),\n","        marker=dict(size=8)\n","    ))\n","\n","    fig.update_layout(\n","        title=f\"Metode Elbow (Normalisasi: {normalization})\",\n","        xaxis_title=\"Jumlah Kluster\",\n","        yaxis_title=\"WCSS\",\n","        showlegend=True,\n","        xaxis=dict(tickmode='linear', dtick=1),\n","        yaxis=dict(gridcolor='lightgray'),\n","        plot_bgcolor='white',\n","        height=500\n","    )\n","\n","    return fig\n","\n","def evaluate_features(data: pd.DataFrame, n_clusters: int = 4, normalization: str = \"Min-Max Scaling\") -> Tuple[List[str], float, float]:\n","    \"\"\"Evaluasi fitur berdasarkan Skor Silhouette dan Indeks Davies-Bouldin tanpa menampilkan baseline.\"\"\"\n","    try:\n","        data_normalized, _ = normalize_data(data.values, normalization)\n","        data_normalized = pd.DataFrame(data_normalized, columns=data.columns)\n","\n","        baseline_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(data_normalized)\n","        baseline_sil = silhouette_score(data_normalized, baseline_kmeans.labels_)\n","        baseline_db = davies_bouldin_score(data_normalized, baseline_kmeans.labels_)\n","\n","        result = []\n","        for fitur in data_normalized.columns:\n","            subset = data_normalized.drop(columns=[fitur])\n","            labels = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit_predict(subset)\n","            sil = silhouette_score(subset, labels)\n","            db = davies_bouldin_score(subset, labels)\n","            result.append({\n","                'fitur_dihapus': fitur,\n","                'silhouette': sil,\n","                'davies_bouldin': db\n","            })\n","        df_result = pd.DataFrame(result).sort_values(by=['silhouette', 'davies_bouldin'], ascending=[False, True])\n","        st.dataframe(df_result, height=300, use_container_width=True)\n","\n","        max_features_to_remove = len(data.columns) - 6\n","        fitur_dihapus = df_result[\n","            (df_result['silhouette'] > baseline_sil) &\n","            (df_result['davies_bouldin'] < baseline_db)\n","        ]['fitur_dihapus'].head(max_features_to_remove).tolist()\n","\n","        fitur_final = [f for f in data.columns if f not in fitur_dihapus]\n","\n","        if len(fitur_final) < 4:\n","            st.warning(\"Seleksi fitur menghasilkan kurang dari 4 fitur. Memilih kombinasi terbaik...\")\n","            fitur_dihapus = df_result['fitur_dihapus'].iloc[:(len(data.columns) - 6)].tolist()\n","            fitur_final = [f for f in data.columns if f not in fitur_dihapus]\n","\n","        st.write(f\"*Fitur yang dihapus:* {fitur_dihapus}\")\n","        st.write(f\"*Fitur tersisa:* {fitur_final}\")\n","        st.write(f\"*Jumlah fitur tersisa:* {len(fitur_final)}\")\n","\n","        data_final = data[fitur_final]\n","        data_final_normalized, _ = normalize_data(data_final.values, normalization)\n","        kmeans_final = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(data_final_normalized)\n","        labels_final = kmeans_final.labels_\n","        sil_final = silhouette_score(data_final_normalized, labels_final)\n","        db_final = davies_bouldin_score(data_final_normalized, labels_final)\n","\n","        return fitur_final, sil_final, db_final\n","    except Exception as e:\n","        st.error(f\"[Error] Gagal melakukan seleksi fitur: {str(e)}\")\n","        return data.columns.tolist(), 0.0, float('inf')\n","\n","def correlation_filtering_auto(data: pd.DataFrame, thresholds: Optional[List[float]] = None, n_clusters: int = 4, normalization: str = \"Min-Max Scaling\") -> Tuple[List[str], float, float]:\n","    \"\"\"Lakukan seleksi fitur otomatis menggunakan penyaringan korelasi tanpa menampilkan baseline.\"\"\"\n","    if thresholds is None:\n","        thresholds = np.arange(0.5, 0.91, 0.05)\n","\n","    data_normalized, _ = normalize_data(data.values, normalization)\n","    data_normalized = pd.DataFrame(data_normalized, columns=data.columns)\n","\n","    hasil_evaluasi = []\n","    st.write(\"*Mengevaluasi berbagai threshold korelasi...*\")\n","    for threshold in thresholds:\n","        try:\n","            corr_matrix = data_normalized.corr().abs()\n","            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","            to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n","            data_filtered = data.drop(columns=to_drop)\n","\n","            if data_filtered.shape[1] < 4:\n","                st.write(f\"*Threshold:* {threshold:.2f} | *Dropped:* {len(to_drop)} | *Fitur Kurang dari 4*\")\n","                continue\n","\n","            data_filtered_normalized, _ = normalize_data(data_filtered.values, normalization)\n","            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n","            labels = kmeans.fit_predict(data_filtered_normalized)\n","            sil = silhouette_score(data_filtered_normalized, labels)\n","            db = davies_bouldin_score(data_filtered_normalized, labels)\n","\n","            hasil_evaluasi.append({\n","                'threshold': threshold,\n","                'n_features': data_filtered.shape[1],\n","                'n_dropped': len(to_drop),\n","                'silhouette': sil,\n","                'davies_bouldin': db\n","            })\n","\n","            st.write(f\"*Threshold:* {threshold:.2f} | *Dropped:* {len(to_drop)} | *S:* {sil:.3f} | *DBI:* {db:.3f}\")\n","        except Exception as e:\n","            st.write(f\"*Threshold:* {threshold:.2f} | *Error:* {str(e)}\")\n","\n","    df_eval = pd.DataFrame(hasil_evaluasi)\n","    st.dataframe(df_eval, height=300, use_container_width=True)\n","\n","    if df_eval.empty:\n","        st.warning(\"Tidak ada threshold yang menghasilkan fitur yang valid. Menggunakan semua fitur...\")\n","        fitur_final = data.columns.tolist()\n","        data_final = data.copy()\n","    else:\n","        df_eval['total_score'] = df_eval['silhouette'] - df_eval['davies_bouldin']\n","        best_threshold = df_eval.sort_values(by='total_score', ascending=False).iloc[0]['threshold']\n","        st.write(f\"\\n*Threshold terbaik berdasarkan metrik:* {best_threshold:.2f}\")\n","        corr_matrix = data_normalized.corr().abs()\n","        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","        to_drop = [column for column in upper.columns if any(upper[column] > best_threshold)]\n","        data_final = data.drop(columns=to_drop)\n","        fitur_final = data_final.columns.tolist()\n","\n","    try:\n","        data_final_normalized, _ = normalize_data(data_final.values, normalization)\n","        kmeans_final = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(data_final_normalized)\n","        labels_final = kmeans_final.labels_\n","        sil_final = silhouette_score(data_final_normalized, labels_final)\n","        db_final = davies_bouldin_score(data_final_normalized, labels_final)\n","    except Exception as e:\n","        st.error(f\"[Error] Gagal menghitung metrik akhir: {str(e)}\")\n","        sil_final, db_final = 0.0, float('inf')\n","\n","    st.write(f\"\\n*Evaluasi Akhir (Threshold = {best_threshold}):*\")\n","    st.write(f\"*Jumlah fitur tersisa:* {data_final.shape[1]}\")\n","    st.write(f\"*Silhouette Score:* {sil_final:.3f}\")\n","    st.write(f\"*Davies-Bouldin Index:* {db_final:.3f}\")\n","\n","    return fitur_final, sil_final, db_final\n","\n","def generate_recommendations(df: pd.DataFrame, data_final: pd.DataFrame, labels: np.ndarray, silhouette: Optional[float],\n","                            db_index: Optional[float], k_value: int, outlier_counts: np.ndarray, cluster_counts: np.ndarray,\n","                            outliers: np.ndarray, outlier_distances: np.ndarray, cleaned_indices: np.ndarray) -> List[str]:\n","    \"\"\"Hasilkan rekomendasi berdasarkan hasil clustering dan analisis outlier.\"\"\"\n","    recommendations = []\n","    n_samples = len(labels)\n","    outlier_percentages = [outlier_counts[i] / cluster_counts[i] * 100 if cluster_counts[i] > 0 else 0 for i in range(k_value)]\n","    max_outlier_cluster = np.argmax(outlier_percentages) if len(outlier_percentages) > 0 else -1\n","    max_outlier_percentage = max(outlier_percentages) if len(outlier_percentages) > 0 else 0\n","\n","    if max_outlier_cluster >= 0 and max_outlier_percentage > 0:\n","        recommendations.append(\n","            f\"*Rekomendasi 1:* Periksa Klaster {max_outlier_cluster} karena memiliki persentase outlier tertinggi ({max_outlier_percentage:.2f}%).\"\n","        )\n","    else:\n","        recommendations.append(\"*Rekomendasi 1:* Tidak ada klaster dengan outlier signifikan.\")\n","\n","    if len(outliers) > 0 and max_outlier_cluster >= 0:\n","        outlier_df = pd.DataFrame({\n","            \"Index\": outliers,\n","            \"Nama Item\": [df.iloc[cleaned_indices[idx]]['Description'] if 'Description' in df.columns and cleaned_indices[idx] < len(df) else f\"Item_{cleaned_indices[idx]}\" for idx in outliers],\n","            \"Klaster\": labels[outliers],\n","            \"Jarak ke Centroid\": [f\"{d:.3f}\" for d in outlier_distances]\n","        })\n","        cluster_outliers = outlier_df[outlier_df[\"Klaster\"] == max_outlier_cluster].head(3)\n","        outlier_items = \", \".join(cluster_outliers[\"Nama Item\"].values) if not cluster_outliers.empty else \"Tidak ada item\"\n","        recommendations.append(\n","            f\"*Rekomendasi 2:* Evaluasi ulang item outlier di Klaster {max_outlier_cluster} (contoh: {outlier_items}).\"\n","        )\n","    else:\n","        recommendations.append(\"*Rekomendasi 2:* Tidak ada outlier signifikan untuk dievaluasi.\")\n","\n","    if silhouette is not None and silhouette < 0.4:\n","        recommendations.append(\n","            f\"*Rekomendasi 3:* Silhouette Score ({silhouette:.4f}) di bawah 0.4, menunjukkan kualitas clustering suboptimal. \"\n","            f\"Pertimbangkan menyesuaikan K (saat ini {k_value}) dengan K lebih besar (misalnya, {k_value + 1}).\"\n","        )\n","    else:\n","        recommendations.append(\"*Rekomendasi 3:* Kualitas clustering memadai berdasarkan Silhouette Score.\")\n","\n","    if db_index is not None and db_index > 1.0:\n","        recommendations.append(\n","            f\"*Rekomendasi 4:* Davies-Bouldin Index ({db_index:.4f}) di atas 1.0, menunjukkan pemisahan klaster kurang optimal.\"\n","        )\n","    else:\n","        recommendations.append(\"*Rekomendasi 4:* Pemisahan klaster cukup baik berdasarkan Davies-Bouldin Index.\")\n","\n","    if len(outliers) > 0:\n","        example_outlier = outlier_df.iloc[0] if not outlier_df.empty else None\n","        if example_outlier is not None:\n","            item_name = example_outlier[\"Nama Item\"]\n","            cluster = example_outlier[\"Klaster\"]\n","            recommendations.append(\n","                f\"*Rekomendasi 5:* Gunakan pengetahuan domain untuk menangani outlier seperti '{item_name}' di Klaster {cluster}.\"\n","            )\n","        else:\n","            recommendations.append(\"*Rekomendasi 5:* Tidak ada outlier untuk dianalisis dengan pengetahuan domain.\")\n","    else:\n","        recommendations.append(\"*Rekomendasi 5:* Tidak ada outlier untuk dianalisis dengan pengetahuan domain.\")\n","\n","    if len(outliers) > 0:\n","        outlier_with_high_distance = outlier_df.iloc[outlier_df[\"Jarak ke Centroid\"].astype(float).idxmax()] if not outlier_df.empty else None\n","        if outlier_with_high_distance is not None:\n","            item_name = outlier_with_high_distance[\"Nama Item\"]\n","            recommendations.append(\n","                f\"*Rekomendasi 6:* Tingkatkan pengumpulan data untuk item seperti '{item_name}' (jarak ke centroid: {outlier_with_high_distance['Jarak ke Centroid']}).\"\n","            )\n","        else:\n","            recommendations.append(\"*Rekomendasi 6:* Tidak ada outlier signifikan untuk meningkatkan pengumpulan data.\")\n","    else:\n","        recommendations.append(\"*Rekomendasi 6:* Tidak ada outlier signifikan untuk meningkatkan pengumpulan data.\")\n","\n","    total_outlier_percentage = len(outliers) / n_samples * 100 if n_samples > 0 else 0\n","    if total_outlier_percentage > 5:\n","        recommendations.append(\n","            f\"*Rekomendasi 7:* Persentase outlier total ({total_outlier_percentage:.2f}%) lebih dari 5%. \"\n","            \"Pertimbangkan metode normalisasi lain seperti RobustScaler.\"\n","        )\n","    else:\n","        recommendations.append(\"*Rekomendasi 7:* Persentase outlier total cukup rendah.\")\n","\n","    if max_outlier_percentage > 5:\n","        recommendations.append(\n","            f\"*Rekomendasi 8:* Klaster {max_outlier_cluster} memiliki outlier tinggi ({max_outlier_percentage:.2f}%), \"\n","            \"yang dapat mengurangi interpretabilitas klaster.\"\n","        )\n","    else:\n","        recommendations.append(\"*Rekomendasi 8:* Interpretabilitas klaster baik berdasarkan distribusi outlier.\")\n","\n","    if len(outliers) > 0:\n","        recommendations.append(\n","            \"*Rekomendasi 9:* Outlier mungkin mewakili item unik atau data salah label. \"\n","            \"Tinjau outlier secara manual untuk memastikan akurasi data.\"\n","        )\n","    else:\n","        recommendations.append(\"*Rekomendasi 9:* Tidak ada outlier yang terdeteksi, tidak perlu tinjauan manual.\")\n","\n","    if max_outlier_percentage > 5:\n","        recommendations.append(\n","            f\"*Rekomendasi 10:* Klaster {max_outlier_cluster} memiliki persentase outlier tinggi ({max_outlier_percentage:.2f}%). \"\n","            f\"Pertimbangkan untuk meningkatkan K menjadi {k_value + 1} untuk mengelompokkan outlier ke klaster baru.\"\n","        )\n","    else:\n","        recommendations.append(\n","            f\"*Rekomendasi 10:* Persentase outlier di semua klaster di bawah 5%. Tidak perlu menambah jumlah klaster (K={k_value}).\"\n","        )\n","\n","    return recommendations\n","\n","# --- Main Application ---\n","def main():\n","    \"\"\"Fungsi utama untuk menjalankan aplikasi clustering Streamlit.\"\"\"\n","    st.title(\"Clustering dengan K-Means\")\n","\n","    # Sidebar untuk input\n","    with st.sidebar:\n","        st.header(\"Konfigurasi Clustering\")\n","        st.markdown(\"*Pilih parameter untuk analisis clustering:*\")\n","        normalization = st.selectbox(\n","            \"Pilih Normalisasi\",\n","            [\"Min-Max Scaling\", \"Z-Score\"],\n","            help=\"Min-Max Scaling: [0,1]. Z-Score: mean=0, std=1.\"\n","        )\n","        feature_method = st.selectbox(\n","            \"Pilih Metode Seleksi Fitur\",\n","            [\"Baseline\", \"Silhouette + Davies-Bouldin\", \"Threshold Correlation\"],\n","            help=\"Baseline: Fitur manual. Silhouette + DB: Berdasarkan metrik clustering. Threshold Correlation: Berdasarkan korelasi.\"\n","        )\n","        uploaded_file = st.file_uploader(\n","            \"Unggah file CSV (opsional)\",\n","            type=[\"csv\"],\n","            help=\"Unggah CSV dengan kolom numerik. Kolom 'Description' opsional untuk interpretasi.\"\n","        )\n","\n","    # Main content\n","    st.subheader(\"1. Memuat Dataset\")\n","    try:\n","        if uploaded_file is not None:\n","            df = pd.read_csv(uploaded_file)\n","            st.write(\"Dataset diganti dengan file yang diunggah.\")\n","        else:\n","            st.write(\"Memuat dataset default food.csv dari Kaggle...\")\n","            dataset_path = kagglehub.dataset_download(\"shrutisaxena/food-nutrition-dataset\")\n","            file_path = os.path.join(dataset_path, \"food.csv\")\n","            df = pd.read_csv(file_path)\n","            st.write(f\"Dataset default berhasil dimuat dari: {file_path}\")\n","\n","        if df.empty:\n","            st.error(\"Dataset kosong! Silakan unggah dataset yang valid atau periksa dataset default.\")\n","            st.stop()\n","\n","        # Tampilkan dataset yang dimuat\n","        st.subheader(\"2. Data yang Dimuat\")\n","        st.write(\"*Data yang dimuat:*\")\n","        st.dataframe(df, height=300, use_container_width=True)\n","\n","        # Tampilkan statistik deskriptif\n","        st.write(\"*Statistik Deskriptif Data:*\")\n","        st.dataframe(df.describe(), height=300, use_container_width=True)\n","\n","        # -----------------------------------------------\n","        # 3. TARGET DATA\n","        st.subheader(\"3. Target Data\")\n","        if uploaded_file is None:  # Dataset bawaan dari Kaggle\n","            kolom_fitur = [\n","                'Data.Carbohydrate', 'Data.Protein', 'Data.Fat.Total Lipid', 'Data.Kilocalories',\n","                'Data.Fiber', 'Data.Sugar Total', 'Data.Major Minerals.Calcium',\n","                'Data.Major Minerals.Iron', 'Data.Vitamins.Vitamin C',\n","                'Data.Vitamins.Vitamin E', 'Data.Major Minerals.Sodium',\n","                'Data.Cholesterol'\n","            ]\n","            if not all(col in df.columns for col in kolom_fitur):\n","                st.error(\"Beberapa kolom fitur tidak ditemukan dalam dataset Kaggle. Periksa nama kolom.\")\n","                st.stop()\n","            data_pilih_fitur = df[kolom_fitur].copy()\n","            data_deskripsi = df[['Description']].copy() if 'Description' in df.columns else pd.DataFrame()\n","            st.write(f\"Ukuran dataset setelah pemilihan fitur: {data_pilih_fitur.shape} (baris, kolom)\")\n","            st.write(\"*Fitur yang digunakan untuk dataset Kaggle:*\", kolom_fitur)\n","        else:  # Dataset yang diunggah\n","            numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n","            if len(numeric_columns) < 2:\n","                st.error(\"Dataset harus memiliki setidaknya 2 kolom numerik untuk clustering!\")\n","                st.stop()\n","            selected_features = st.multiselect(\n","                \"Pilih fitur yang akan digunakan\",\n","                numeric_columns,\n","                default=numeric_columns[:min(len(numeric_columns), 12)],\n","                help=\"Pilih minimal 2 fitur numerik.\"\n","            )\n","            if not selected_features or len(selected_features) < 2:\n","                st.error(\"Pilih setidaknya 2 fitur untuk clustering!\")\n","                st.stop()\n","            data_pilih_fitur = df[selected_features].copy()\n","            data_deskripsi = df[['Description']].copy() if 'Description' in df.columns else pd.DataFrame()\n","            st.write(f\"Ukuran dataset setelah pemilihan fitur: {data_pilih_fitur.shape} (baris, kolom)\")\n","            st.write(\"*Fitur yang dipilih pengguna:*\", selected_features)\n","\n","        # -----------------------------------------------\n","        # 4. PEMBERSIHAN DATA\n","        st.subheader(\"4. Pembersihan Data\")\n","        initial_rows = data_pilih_fitur.shape[0]\n","        st.write(f\"*Jumlah baris awal:* {initial_rows}\")\n","\n","        # Periksa duplikat awal\n","        initial_duplicates = data_pilih_fitur.duplicated().sum()\n","        st.write(f\"*Jumlah duplikat awal dalam fitur terpilih:* {initial_duplicates}\")\n","\n","        st.write(\"Mengkonversi kolom ke tipe numerik...\")\n","        for kolom in data_pilih_fitur.columns:\n","            data_pilih_fitur[kolom] = pd.to_numeric(data_pilih_fitur[kolom], errors='coerce')\n","\n","        data_bersih = data_pilih_fitur.dropna()\n","        nan_removed = initial_rows - data_bersih.shape[0]\n","        st.write(f\"Jumlah baris yang dihapus karena NaN: {nan_removed}\")\n","\n","        data_bersih = data_bersih[(data_bersih >= 0).all(axis=1)]\n","        neg_removed = initial_rows - nan_removed - data_bersih.shape[0]\n","        st.write(f\"Jumlah baris yang dihapus karena nilai negatif: {neg_removed}\")\n","\n","        data_bersih_before_dedup = data_bersih.copy()\n","        data_bersih = data_bersih.drop_duplicates()\n","        dup_removed = data_bersih_before_dedup.shape[0] - data_bersih.shape[0]\n","        st.write(f\"Jumlah baris yang dihapus karena duplikat: {dup_removed}\")\n","\n","        st.write(f\"*Jumlah baris setelah pembersihan:* {data_bersih.shape[0]}\")\n","        st.dataframe(data_bersih, height=300, use_container_width=True)\n","\n","        if data_bersih.empty:\n","            st.error(\"Dataset kosong setelah pembersihan.\")\n","            st.stop()\n","\n","        if data_bersih.shape[1] < 2:\n","            st.error(\"Dataset harus memiliki setidaknya 2 fitur numerik setelah pembersihan!\")\n","            st.stop()\n","\n","        cleaned_indices = data_bersih.index\n","        X = data_bersih.values\n","        X_normalized_elbow, _ = normalize_data(X, normalization)\n","\n","        st.write(\"\\n*Validasi Normalisasi:*\")\n","        if normalization == \"Min-Max Scaling\":\n","            st.write(\"Rentang data setelah normalisasi (harus [0,1]):\")\n","            st.write(f\"Min: {X_normalized_elbow.min():.2f}, Max: {X_normalized_elbow.max():.2f}\")\n","        else:\n","            st.write(\"Statistik data setelah normalisasi (harus mean~0, std~1):\")\n","            st.write(f\"Mean: {X_normalized_elbow.mean():.2e}, Std: {X_normalized_elbow.std():.2f}\")\n","\n","        # -----------------------------------------------\n","        # 5. VALIDASI JUMLAH KLASTER (ELBOW METHOD)\n","        st.subheader(\"5. Validasi Jumlah Klaster (Elbow Method)\")\n","        st.write(\"*Menghitung WCSS untuk menentukan jumlah klaster optimal...*\")\n","        elbow_fig = plot_elbow_method(X_normalized_elbow, normalization)\n","        st.plotly_chart(elbow_fig)\n","        st.write(\"*Instruksi:* Lihat plot di atas untuk menentukan 'elbow point'.\")\n","\n","        k_value = st.number_input(\"Masukkan jumlah K (minimal 2)\", min_value=2, max_value=10, value=4, step=1)\n","\n","        if not st.button(\"Lanjutkan Clustering dengan K yang Dipilih\"):\n","            st.stop()\n","\n","        # -----------------------------------------------\n","        # 6. SELEKSI FITUR\n","        st.subheader(\"6. Seleksi Fitur\")\n","        data_final = data_bersih.copy()\n","        fitur_final = data_bersih.columns.tolist()\n","\n","        if feature_method == \"Baseline\":\n","            data_normalized, _ = normalize_data(data_bersih.values, normalization)\n","            baseline_kmeans = KMeans(n_clusters=k_value, random_state=42, n_init=10).fit(data_normalized)\n","            baseline_sil = silhouette_score(data_normalized, baseline_kmeans.labels_)\n","            baseline_db = davies_bouldin_score(data_normalized, baseline_kmeans.labels_)\n","            st.write(f\"*Baseline (Fitur Terpilih) -> Silhouette: {baseline_sil:.3f}, Davies-Bouldin: {baseline_db:.3f}*\")\n","\n","        if feature_method == \"Baseline\":\n","            st.write(\"*Metode: Baseline*\")\n","            if st.button(\"Lakukan Seleksi Fitur Baseline\"):\n","                fitur_sil_db, sil_sil_db, db_sil_db = evaluate_features(data_bersih, n_clusters=k_value, normalization=normalization)\n","                fitur_corr, sil_corr, db_corr = correlation_filtering_auto(data_bersih, n_clusters=k_value, normalization=normalization)\n","                if sil_sil_db > sil_corr or (sil_sil_db == sil_corr and db_sil_db < db_corr):\n","                    st.write(\"*Metode Terpilih: Silhouette + Davies-Bouldin*\")\n","                    fitur_final = fitur_sil_db\n","                else:\n","                    st.write(\"*Metode Terpilih: Threshold Correlation*\")\n","                    fitur_final = fitur_corr\n","        elif feature_method == \"Silhouette + Davies-Bouldin\":\n","            st.write(\"*Metode: Silhouette + Davies-Bouldin Index*\")\n","            fitur_final, _, _ = evaluate_features(data_bersih, n_clusters=k_value, normalization=normalization)\n","        elif feature_method == \"Threshold Correlation\":\n","            st.write(\"*Metode: Threshold Correlation*\")\n","            fitur_final, _, _ = correlation_filtering_auto(data_bersih, n_clusters=k_value, normalization=normalization)\n","\n","        data_final = data_bersih[fitur_final]\n","        st.write(f\"*Jumlah fitur yang digunakan:* {len(fitur_final)}\")\n","        st.write(f\"*Fitur yang digunakan:* {fitur_final}\")\n","\n","        # -----------------------------------------------\n","        # 7. TRANSFORMASI DATA\n","        st.subheader(\"7. Transformasi Data\")\n","        st.write(\"Menangani outlier dengan pembatasi pada persentil ke-99...\")\n","        for kolom in data_final.columns:\n","            batas_atas = data_final[kolom].quantile(0.99)\n","            data_final[kolom] = data_final[kolom].clip(upper=batas_atas)\n","\n","        X = data_final.values\n","        X_normalized, scaler = normalize_data(X, normalization)\n","        labels, model = perform_clustering(X_normalized, k_value)\n","        if labels.size == 0:\n","            st.error(\"Clustering gagal menghasilkan label. Periksa data atau parameter.\")\n","            st.stop()\n","        data_final['Cluster'] = labels\n","\n","        n_clusters = get_clustering_diagnostics(labels)\n","        silhouette = calculate_silhouette_score(X_normalized, labels)\n","        db_index = calculate_davies_bouldin_score(X_normalized, labels)\n","\n","        # -----------------------------------------------\n","        # 8. HASIL CLUSTERING\n","        st.subheader(\"8. Hasil Clustering\")\n","        st.write(f\"*Algoritma*: K-Means\")\n","        st.write(f\"*Normalisasi*: {normalization}\")\n","        st.write(f\"*Jumlah Klaster (K)*: {k_value}\")\n","        st.write(f\"*Jumlah Klaster Terbentuk*: {n_clusters}\")\n","        outlier_counts, cluster_counts, outliers, outlier_distances = analisis_kmeans(X_normalized, labels, model)\n","\n","        st.write(\"*Akurasi (Evaluasi):*\")\n","        if silhouette is not None:\n","            st.write(f\"- Silhouette Score: {silhouette:.4f}\")\n","        else:\n","            st.write(\"- Silhouette Score: Tidak dapat dihitung\")\n","        if db_index is not None:\n","            st.write(f\"- Davies-Bouldin Index: {db_index:.4f}\")\n","        else:\n","            st.write(\"- Davies-Bouldin Index: Tidak dapat dihitung\")\n","\n","        # -----------------------------------------------\n","        # 9. SILHOUETTE ANALYSIS\n","        st.subheader(\"9. Silhouette Analysis\")\n","        fig, error_msg = plot_silhouette_analysis(X_normalized, labels, normalization)\n","        if fig is not None:\n","            st.plotly_chart(fig)\n","        else:\n","            st.warning(f\"Silhouette Analysis tidak dapat ditampilkan: {error_msg}\")\n","\n","        # -----------------------------------------------\n","        # 10. INSTANCE ERROR (OUTLIER) ANALYSIS\n","        st.subheader(\"10. Instance Error (Outlier) Analysis\")\n","        st.write(\"\\n*Detail Instance Error (Outlier):*\")\n","        distances = np.zeros(len(labels))\n","        cluster_centers = model.cluster_centers_\n","        for i in range(len(labels)):\n","            cluster = labels[i]\n","            distances[i] = np.linalg.norm(X_normalized[i] - cluster_centers[cluster])\n","        threshold = np.percentile(distances, 95) if len(distances) > 0 else np.inf\n","        outliers = np.where(distances > threshold)[0]\n","\n","        if len(outliers) > 0:\n","            st.write(\"*Daftar item yang dianggap outlier:*\")\n","            outlier_details = []\n","            unique_clusters = range(n_clusters)\n","            for idx in outliers:\n","                orig_idx = cleaned_indices[idx]\n","                dists = []\n","                for cluster in unique_clusters:\n","                    cluster_points = X_normalized[labels == cluster]\n","                    if len(cluster_points) > 0:\n","                        center = np.mean(cluster_points, axis=0)\n","                        dist = np.linalg.norm(X_normalized[idx] - center)\n","                        dists.append(dist)\n","                nearest_cluster = unique_clusters[np.argmin(dists)] if dists else -1\n","                distance = min(dists) if dists else float('nan')\n","                item_name = df.iloc[orig_idx]['Description'] if 'Description' in df.columns and orig_idx < len(df) else f\"Item_{orig_idx}\"\n","                outlier_details.append({\n","                    \"Index\": idx,\n","                    \"Nama Item\": item_name,\n","                    \"Klaster\": nearest_cluster,\n","                    \"Jarak ke Centroid\": f\"{distance:.3f}\"\n","                })\n","\n","            outlier_df = pd.DataFrame(outlier_details)\n","            st.dataframe(outlier_df, height=300, use_container_width=True)\n","\n","            outlier_output = io.StringIO()\n","            outlier_df.to_csv(outlier_output, index=False)\n","            st.download_button(\n","                label=\"Download seluruh data outlier sebagai CSV\",\n","                data=outlier_output.getvalue(),\n","                file_name=\"outlier_data.csv\",\n","                mime=\"text/csv\"\n","            )\n","        else:\n","            st.write(\"*Tidak ada outlier yang ditemukan.*\")\n","\n","        # -----------------------------------------------\n","        # 11. INTERPRETASI KUALITATIF: TAMPILAN CONTOH MAKANAN\n","        st.subheader(\"11. Interpretasi Kualitatif: Tampilan Contoh Makanan per Klaster\")\n","        kolom_fitur = data_final.columns.tolist()\n","        if 'Cluster' in kolom_fitur:\n","            kolom_fitur.remove('Cluster')\n","\n","        st.write(\"\\n*Contoh Makanan per Klaster:*\")\n","        unique_clusters = range(k_value)\n","\n","        for cluster in unique_clusters:\n","            st.write(f\"\\n*Cluster {cluster}:*\")\n","            cluster_data = data_final[data_final['Cluster'] == cluster]\n","            display_columns = ['Description'] + kolom_fitur if 'Description' in df.columns else kolom_fitur\n","            if 'Description' in df.columns:\n","                cluster_data_with_desc = cluster_data.copy()\n","                cluster_data_with_desc['Description'] = df.loc[cluster_data.index, 'Description']\n","                st.dataframe(\n","                    cluster_data_with_desc[display_columns],\n","                    height=300,\n","                    use_container_width=True\n","                )\n","            else:\n","                st.dataframe(\n","                    cluster_data[display_columns],\n","                    height=300,\n","                    use_container_width=True\n","                )\n","\n","            cluster_output = io.StringIO()\n","            cluster_data.to_csv(cluster_output, index=False)\n","            st.download_button(\n","                label=f\"Download data Cluster {cluster} sebagai CSV\",\n","                data=cluster_output.getvalue(),\n","                file_name=f\"cluster_{cluster}_data.csv\",\n","                mime=\"text/csv\"\n","            )\n","\n","        # -----------------------------------------------\n","        # 12. REKOMENDASI\n","        st.subheader(\"12. Rekomendasi Berdasarkan Hasil Clustering dan Outlier\")\n","        recommendations = generate_recommendations(\n","            df, data_final, labels, silhouette, db_index, k_value, outlier_counts, cluster_counts, outliers, outlier_distances, cleaned_indices\n","        )\n","        for rec in recommendations:\n","            st.write(rec)\n","\n","        # -----------------------------------------------\n","        # 13. DOWNLOAD HASIL CLUSTERING KESELURUHAN\n","        st.subheader(\"13. Download Hasil Clustering Keseluruhan\")\n","        result_df = df.loc[cleaned_indices].copy()\n","        result_df['Cluster'] = labels\n","        st.dataframe(result_df, height=300, use_container_width=True)\n","        output = io.StringIO()\n","        result_df.to_csv(output, index=False)\n","        st.download_button(\n","            label=\"Download hasil clustering keseluruhan\",\n","            data=output.getvalue(),\n","            file_name=\"clustering_result.csv\",\n","            mime=\"text/csv\"\n","        )\n","    except Exception as e:\n","        st.error(f\"Terjadi kesalahan saat memproses data: {str(e)}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1J83ZJHpayDl","executionInfo":{"status":"ok","timestamp":1749649333096,"user_tz":-420,"elapsed":238,"user":{"displayName":"Rofiq Samanhudi","userId":"07668947446064414586"}},"outputId":"e0a68339-9eff-4ba6-ba10-3ff27653d970"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing clustering_app.py\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","import threading\n","import time\n","import subprocess\n","import socket\n","\n","# Fungsi menunggu sampai port Streamlit terbuka\n","def wait_for_port(host, port, timeout=60):\n","    start_time = time.time()\n","    while time.time() - start_time < timeout:\n","        try:\n","            with socket.create_connection((host, port), timeout=2):\n","                return True\n","        except OSError:\n","            time.sleep(1)\n","    return False\n","\n","# Jalankan Streamlit di thread terpisah\n","def start_streamlit():\n","    subprocess.Popen(['streamlit', 'run', '/content/clustering_app.py'])\n","\n","threading.Thread(target=start_streamlit).start()\n","\n","# Tunggu sampai Streamlit aktif\n","if wait_for_port('localhost', 8501):\n","    print(\"Streamlit sudah siap di port 8501\")\n","else:\n","    print(\"Timeout: Gagal menunggu Streamlit berjalan\")\n","\n","# Cek apakah tunnel ke port 8501 sudah aktif\n","existing_tunnel = None\n","for tunnel in ngrok.get_tunnels():\n","    if \"http://localhost:8501\" in tunnel.config.get(\"addr\", \"\"):\n","        existing_tunnel = tunnel\n","        break\n","\n","# Jika belum ada, buat tunnel baru\n","if not existing_tunnel:\n","    public_url = ngrok.connect(8501, bind_tls=True)\n","else:\n","    public_url = existing_tunnel\n","\n","# Tampilkan URL publik\n","print(f\"\\nAkses aplikasimu di: {public_url.public_url}\")\n","print(\"Jika muncul halaman peringatan ngrok, klik 'Visit Site' untuk melanjutkan ke aplikasi Streamlit.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsFU2zOl3Yy3","executionInfo":{"status":"ok","timestamp":1749649336060,"user_tz":-420,"elapsed":2961,"user":{"displayName":"Rofiq Samanhudi","userId":"07668947446064414586"}},"outputId":"6203fd81-3911-4068-a80a-92641656478d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit sudah siap di port 8501\n","\n","Akses aplikasimu di: https://98b0-34-125-188-6.ngrok-free.app\n","Jika muncul halaman peringatan ngrok, klik 'Visit Site' untuk melanjutkan ke aplikasi Streamlit.\n"]}]}]}